# AI Gateway ‚Äî 3-Tier AI Failover (Copilot ‚Üí Cloud ‚Üí Local) + RAG

A production-ready AI gateway microservice that routes queries through a **3-tier failover chain** and supports **RAG (Retrieval-Augmented Generation)** ‚Äî designed for a single laptop without a GPU.

| Priority | Tier        | Backend               | When used                            |
|----------|-------------|-----------------------|--------------------------------------|
| 1        | **Primary** | GitHub Copilot        | Default ‚Äî when Copilot is reachable  |
| 2        | **Secondary** | Cloud LLM (OpenAI) | When Copilot circuit is OPEN         |
| 3        | **Fallback** | Local Ollama          | When both cloud tiers are OPEN / offline |

**Design goals**:  üíª Single laptop  ‚ùå No GPU required  üåê Cloud-first  üõë Local only on internet cutoff  ‚ö° Lightweight  üß© Language-agnostic REST API

Each cloud tier (Copilot & OpenAI) has an independent **circuit breaker** that opens after N consecutive failures and re-probes after a configurable timeout.

```
             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
             ‚îÇ   Your App (any language)‚îÇ
             ‚îÇ  Python ¬∑ Java ¬∑ C# ¬∑ Go ‚îÇ
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ HTTP REST
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ       AI Gateway        ‚îÇ
              ‚îÇ  POST /ai/query         ‚îÇ
              ‚îÇ  POST /ai/query/rag     ‚îÇ
              ‚îÇ  GET  /health           ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ 3-tier failover
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ               ‚îÇ               ‚îÇ
    ü§ñ Copilot      üåê Cloud LLM    üíª Local LLM
    (Primary)       (Secondary)     (Fallback)
    GitHub          OpenAI, etc.    Ollama (CPU)
```

## üìÅ Project Structure

```
ai-gateway/
‚îú‚îÄ‚îÄ copilot_client.py   # GitHub Copilot client (primary)
‚îú‚îÄ‚îÄ cloud_client.py     # Cloud LLM client (secondary ‚Äî OpenAI)
‚îú‚îÄ‚îÄ local_client.py     # Local LLM client (tertiary ‚Äî Ollama)
‚îú‚îÄ‚îÄ router.py           # 3-tier circuit-breaker router
‚îú‚îÄ‚îÄ ai_gateway.py       # FastAPI REST service (plain + RAG endpoints)
‚îú‚îÄ‚îÄ Dockerfile          # Container image
‚îú‚îÄ‚îÄ docker-compose.yml  # Gateway + Ollama stack
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ setup-mac.sh        # macOS one-command setup
‚îÇ   ‚îú‚îÄ‚îÄ setup-linux.sh      # Linux / Debian / Ubuntu one-command setup
‚îÇ   ‚îú‚îÄ‚îÄ setup-wsl.sh        # WSL 2 one-command setup
‚îÇ   ‚îî‚îÄ‚îÄ setup-windows.ps1   # Windows PowerShell one-command setup
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ client.py       # Python client example
‚îÇ   ‚îú‚îÄ‚îÄ Client.java     # Java client example
‚îÇ   ‚îú‚îÄ‚îÄ Client.cs       # C# client example
‚îÇ   ‚îî‚îÄ‚îÄ client.go       # Go client example
‚îú‚îÄ‚îÄ .vscode/
‚îÇ   ‚îî‚îÄ‚îÄ settings.json   # VS Code + Copilot settings
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ test_gateway.py # Unit & integration tests (36 tests)
```

## üöÄ Local Deployment (no Docker)

One-command setup scripts are provided for every major local environment.
Each script: installs Python, creates a virtual environment, installs dependencies,
installs Ollama (CPU-only local fallback), and generates a `.env` template.

### Prerequisites (all platforms)

| Requirement | Minimum version | Notes |
|---|---|---|
| Python | 3.11 | Installed by the script if missing |
| pip | latest | Upgraded automatically |
| Ollama | latest | Installed by the script; runs CPU-only |
| GitHub Copilot token | ‚Äî | `gh auth login --scopes copilot && gh auth token` |
| OpenAI API key | ‚Äî | Optional; enables cloud secondary tier |

---

### üçé macOS (Intel & Apple Silicon)

```bash
cd ai-gateway
chmod +x scripts/setup-mac.sh
./scripts/setup-mac.sh
```

Then start the gateway:

```bash
source .venv/bin/activate
set -a && source .env && set +a        # load tokens from .env
uvicorn ai_gateway:app --host 0.0.0.0 --port 8000 --reload
```

> Requires [Homebrew](https://brew.sh). The script installs it automatically if absent.

---

### üêß Linux / Debian / Ubuntu

```bash
cd ai-gateway
chmod +x scripts/setup-linux.sh
./scripts/setup-linux.sh
```

Then start the gateway:

```bash
source .venv/bin/activate
set -a && source .env && set +a
uvicorn ai_gateway:app --host 0.0.0.0 --port 8000 --reload
```

> Requires `sudo` (for `apt-get`). The Ollama installer registers a **systemd** service
> so Ollama restarts automatically on reboot.

---

### ü™ü WSL 2 (Windows Subsystem for Linux)

Open a **WSL terminal** (Ubuntu 22.04 or 24.04 recommended):

```bash
cd ai-gateway
chmod +x scripts/setup-wsl.sh
./scripts/setup-wsl.sh
```

Then start the gateway inside WSL:

```bash
source .venv/bin/activate
set -a && source .env && set +a
uvicorn ai_gateway:app --host 0.0.0.0 --port 8000 --reload
```

Access from Windows browser / `curl`: **`http://localhost:8000`**
(WSL 2 auto-forwards ports to Windows on Windows 11 / WSL 2.0+).

> **Tip ‚Äî if ports are not auto-forwarded on Windows 10:**
> ```powershell
> # Run once in an elevated PowerShell on the Windows host
> netsh interface portproxy add v4tov4 listenport=8000 listenaddress=0.0.0.0 `
>       connectport=8000 connectaddress=$(wsl hostname -I).Split()[0]
> ```

---

### ü™ü Windows (native PowerShell)

Open **PowerShell** (no admin required) and run:

```powershell
Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned -Force
cd ai-gateway
.\scripts\setup-windows.ps1
```

Then start the gateway:

```powershell
.venv\Scripts\Activate.ps1

# Load tokens from .env
Get-Content .env | ForEach-Object {
    if ($_ -match '^\s*([^#][^=]*)=(.+)') {
        [System.Environment]::SetEnvironmentVariable($matches[1].Trim(), $matches[2].Trim(), 'Process')
    }
}

uvicorn ai_gateway:app --host 0.0.0.0 --port 8000 --reload
```

> The script installs Python 3.11 and Ollama via **winget** (built into Windows 10 21H2+ and Windows 11).
> If winget is unavailable, download them manually:
> [python.org/downloads](https://www.python.org/downloads/) ¬∑ [ollama.com/download/windows](https://ollama.com/download/windows)

---

### üê≥ Docker Compose (all platforms)

```bash
GITHUB_COPILOT_TOKEN=<token> OPENAI_API_KEY=sk-... docker compose up
```

The gateway is then available at `http://localhost:8000`.

---

### üîë Getting a GitHub Copilot token

```bash
# Option A: GitHub CLI (recommended ‚Äî works on all platforms)
gh auth login --scopes copilot
gh auth token           # copy this value into GITHUB_COPILOT_TOKEN

# Option B: VS Code extension (token stored automatically)
# Linux/macOS: ~/.config/github-copilot/hosts.json
# Windows:     %APPDATA%\GitHub Copilot\hosts.json
# WSL:         /mnt/c/Users/<you>/AppData/Roaming/GitHub Copilot/hosts.json
```

## üîå API Reference

### `POST /ai/query` ‚Äî plain query

```json
{
  "prompt": "What is the capital of France?"
}
```

Response:

```json
{
  "response": "The capital of France is Paris.",
  "backend": "copilot",
  "state": "closed"
}
```

| Field     | Description                                              |
|-----------|----------------------------------------------------------|
| `response`| The AI model's answer                                    |
| `backend` | `"copilot"`, `"cloud"`, or `"local"` ‚Äî which tier replied |
| `state`   | Primary (Copilot) circuit state: `closed / open / half_open` |

### `POST /ai/query/rag` ‚Äî RAG-augmented query

Grounded responses without a GPU or vector database.  Pass text chunks as
`documents` and the gateway injects them as context before routing through
the 3-tier chain.

```json
{
  "prompt": "What does the document say about refunds?",
  "documents": [
    "Refund policy: customers may return items within 30 days.",
    "No refunds are issued after 30 days from purchase."
  ],
  "max_context_chars": 4000
}
```

Response:

```json
{
  "response": "According to the policy, refunds are available within 30 days of purchase.",
  "backend": "copilot",
  "state": "closed"
}
```

| Field              | Default | Description                                          |
|--------------------|---------|------------------------------------------------------|
| `prompt`           | ‚Äî       | The question to answer                               |
| `documents`        | ‚Äî       | List of text chunks to use as context                |
| `max_context_chars`| `4000`  | Max chars of context injected (fits within model limits) |

**Use cases on a no-GPU laptop**:
- Chat over a local code snippet
- Answer questions about a pasted document
- Ground responses in retrieved database rows
- Summarise meeting notes / tickets

### `GET /health`

Returns availability of all three backends and current Copilot circuit state.

```json
{
  "status": "ok",
  "copilot_available": true,
  "cloud_available": true,
  "local_available": false,
  "circuit_state": "closed"
}
```

### `POST /admin/reset`

Manually reset both circuit breakers to `closed` (Copilot-preferred) state.

## ‚öôÔ∏è Configuration (environment variables)

| Variable               | Default                   | Description                                |
|------------------------|---------------------------|--------------------------------------------|
| `GITHUB_COPILOT_TOKEN` | *(from VS Code config)*   | GitHub Copilot OAuth token (primary)       |
| `COPILOT_MODEL`        | `gpt-4o`                  | Copilot model name                         |
| `CLOUD_PROVIDER`       | `openai`                  | Secondary cloud provider                   |
| `CLOUD_MODEL`          | `gpt-4o-mini`             | Secondary cloud model                      |
| `OPENAI_API_KEY`       | *(required for cloud)*    | OpenAI API key (secondary)                 |
| `LOCAL_MODEL`          | `llama3`                  | Ollama model name (fallback)               |
| `OLLAMA_BASE_URL`      | `http://localhost:11434`  | Ollama service URL                         |
| `FAILURE_THRESHOLD`    | `3`                       | Failures before opening a tier's circuit   |
| `RECOVERY_TIMEOUT`     | `300`                     | Seconds before re-probing a failed tier    |

## üîÑ Circuit Breaker State Machine (per tier)

```
CLOSED ‚îÄ‚îÄ(N failures)‚îÄ‚îÄ‚ñ∫ OPEN ‚îÄ‚îÄ(timeout)‚îÄ‚îÄ‚ñ∫ HALF-OPEN
  ‚ñ≤                                               ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ(success)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

| State       | Behaviour                                             |
|-------------|-------------------------------------------------------|
| `closed`    | Tier healthy ‚Äî requests routed here first             |
| `open`      | Tier down ‚Äî skipped, next tier tried                  |
| `half_open` | Trial request sent to test recovery                   |

## üß© Language-Agnostic ‚Äî Client Examples

The gateway is a plain HTTP REST API ‚Äî call it from any language.
Ready-to-run examples live in `examples/`:

### Python (stdlib ‚Äî no extra packages)

```python
import json, urllib.request

def query(prompt):
    payload = json.dumps({"prompt": prompt}).encode()
    req = urllib.request.Request(
        "http://localhost:8000/ai/query",
        data=payload, headers={"Content-Type": "application/json"}, method="POST"
    )
    with urllib.request.urlopen(req) as r:
        return json.loads(r.read())

def query_rag(prompt, documents):
    payload = json.dumps({"prompt": prompt, "documents": documents}).encode()
    req = urllib.request.Request(
        "http://localhost:8000/ai/query/rag",
        data=payload, headers={"Content-Type": "application/json"}, method="POST"
    )
    with urllib.request.urlopen(req) as r:
        return json.loads(r.read())
```

### Java (stdlib ‚Äî Java 11+)

```java
HttpClient http = HttpClient.newHttpClient();
HttpRequest req = HttpRequest.newBuilder()
    .uri(URI.create("http://localhost:8000/ai/query"))
    .header("Content-Type", "application/json")
    .POST(HttpRequest.BodyPublishers.ofString("{\"prompt\":\"Hello\"}"))
    .build();
String body = http.send(req, HttpResponse.BodyHandlers.ofString()).body();
```

### C# (stdlib ‚Äî .NET 6+)

```csharp
var client = new HttpClient();
var content = new StringContent(
    JsonSerializer.Serialize(new { prompt = "Hello" }),
    Encoding.UTF8, "application/json");
var resp = await client.PostAsync("http://localhost:8000/ai/query", content);
var json = await resp.Content.ReadAsStringAsync();
```

### Go (stdlib)

```go
body, _ := json.Marshal(map[string]string{"prompt": "Hello"})
resp, _ := http.Post("http://localhost:8000/ai/query",
    "application/json", bytes.NewReader(body))
result, _ := io.ReadAll(resp.Body)
```

> See `examples/` for complete working programs in all four languages.

## üíª VS Code + GitHub Copilot

Open this folder in VS Code:

```bash
code ai-gateway/
```

The `.vscode/settings.json` file pre-configures:
- Python test discovery (pytest)
- Copilot inline suggestions enabled
- Correct Python path for module imports

## üß™ Running Tests

```bash
cd ai-gateway
pytest tests/ -v
```

## üèó Hardware Recommendations for Local (Fallback) Models

| RAM      | Suggested model |
|----------|-----------------|
| 8 GB     | `phi3` (3B)     |
| 16 GB    | `llama3` (7B)   |
| 32 GB    | `llama3:13b`    |
| GPU 8 GB+| Any ‚Äî much faster |

## ‚öñÔ∏è Tier Trade-offs

| Feature            | Copilot (Primary) | Cloud (Secondary) | Local (Fallback) |
|--------------------|-------------------|-------------------|------------------|
| Accuracy           | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê            | ‚≠ê‚≠ê‚≠ê‚≠ê              | ‚≠ê‚≠ê‚≠ê              |
| IDE integration    | ‚úÖ Native           | ‚ùå                 | ‚ùå                |
| Internet required  | Yes               | Yes               | No               |
| Cost               | Copilot plan      | API-based         | Hardware-based   |
| Privacy            | GitHub            | External          | Fully private    |
