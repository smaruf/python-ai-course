# AI Gateway â€” 3-Tier AI Failover (Copilot â†’ Cloud â†’ Local) + RAG

A production-ready AI gateway microservice that routes queries through a **3-tier failover chain** and supports **RAG (Retrieval-Augmented Generation)** â€” designed for a single laptop without a GPU.

| Priority | Tier        | Backend               | When used                            |
|----------|-------------|-----------------------|--------------------------------------|
| 1        | **Primary** | GitHub Copilot        | Default â€” when Copilot is reachable  |
| 2        | **Secondary** | Cloud LLM (OpenAI) | When Copilot circuit is OPEN         |
| 3        | **Fallback** | Local Ollama          | When both cloud tiers are OPEN / offline |

**Design goals**:  ğŸ’» Single laptop  âŒ No GPU required  ğŸŒ Cloud-first  ğŸ›‘ Local only on internet cutoff  âš¡ Lightweight  ğŸ§© Language-agnostic REST API

Each cloud tier (Copilot & OpenAI) has an independent **circuit breaker** that opens after N consecutive failures and re-probes after a configurable timeout.

```
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   Your App (any language)â”‚
             â”‚  Python Â· Java Â· C# Â· Go â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTP REST
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚       AI Gateway        â”‚
              â”‚  POST /ai/query         â”‚
              â”‚  POST /ai/query/rag     â”‚
              â”‚  GET  /health           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ 3-tier failover
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚               â”‚               â”‚
    ğŸ¤– Copilot      ğŸŒ Cloud LLM    ğŸ’» Local LLM
    (Primary)       (Secondary)     (Fallback)
    GitHub          OpenAI, etc.    Ollama (CPU)
```

## ğŸ“ Project Structure

```
ai-gateway/
â”œâ”€â”€ copilot_client.py   # GitHub Copilot client (primary)
â”œâ”€â”€ cloud_client.py     # Cloud LLM client (secondary â€” OpenAI)
â”œâ”€â”€ local_client.py     # Local LLM client (tertiary â€” Ollama)
â”œâ”€â”€ router.py           # 3-tier circuit-breaker router
â”œâ”€â”€ ai_gateway.py       # FastAPI REST service (plain + RAG endpoints)
â”œâ”€â”€ Dockerfile          # Container image
â”œâ”€â”€ docker-compose.yml  # Gateway + Ollama stack
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ client.py       # Python client example
â”‚   â”œâ”€â”€ Client.java     # Java client example
â”‚   â”œâ”€â”€ Client.cs       # C# client example
â”‚   â””â”€â”€ client.go       # Go client example
â”œâ”€â”€ .vscode/
â”‚   â””â”€â”€ settings.json   # VS Code + Copilot settings
â””â”€â”€ tests/
    â””â”€â”€ test_gateway.py # Unit & integration tests (36 tests)
```

## ğŸš€ Quick Start

### Option 1 â€” Run locally

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set tokens (at minimum one cloud tier is required)
export GITHUB_COPILOT_TOKEN="<your-copilot-token>"  # primary
export OPENAI_API_KEY="sk-..."                        # secondary fallback

# 3. Start local Ollama (tertiary fallback â€” optional but recommended)
curl -fsSL https://ollama.com/install.sh | sh
ollama run llama3

# 4. Start the gateway
uvicorn ai_gateway:app --reload
```

### Option 2 â€” Docker Compose (recommended)

```bash
GITHUB_COPILOT_TOKEN=<token> OPENAI_API_KEY=sk-... docker compose up
```

The gateway is then available at `http://localhost:8000`.

### Getting a GitHub Copilot token

```bash
# Option A: GitHub CLI (recommended)
gh auth login --scopes copilot
gh auth token

# Option B: VS Code (token stored automatically after installing Copilot extension)
# Read from: ~/.config/github-copilot/hosts.json  (Linux/macOS)
```

## ğŸ”Œ API Reference

### `POST /ai/query` â€” plain query

```json
{
  "prompt": "What is the capital of France?"
}
```

Response:

```json
{
  "response": "The capital of France is Paris.",
  "backend": "copilot",
  "state": "closed"
}
```

| Field     | Description                                              |
|-----------|----------------------------------------------------------|
| `response`| The AI model's answer                                    |
| `backend` | `"copilot"`, `"cloud"`, or `"local"` â€” which tier replied |
| `state`   | Primary (Copilot) circuit state: `closed / open / half_open` |

### `POST /ai/query/rag` â€” RAG-augmented query

Grounded responses without a GPU or vector database.  Pass text chunks as
`documents` and the gateway injects them as context before routing through
the 3-tier chain.

```json
{
  "prompt": "What does the document say about refunds?",
  "documents": [
    "Refund policy: customers may return items within 30 days.",
    "No refunds are issued after 30 days from purchase."
  ],
  "max_context_chars": 4000
}
```

Response:

```json
{
  "response": "According to the policy, refunds are available within 30 days of purchase.",
  "backend": "copilot",
  "state": "closed"
}
```

| Field              | Default | Description                                          |
|--------------------|---------|------------------------------------------------------|
| `prompt`           | â€”       | The question to answer                               |
| `documents`        | â€”       | List of text chunks to use as context                |
| `max_context_chars`| `4000`  | Max chars of context injected (fits within model limits) |

**Use cases on a no-GPU laptop**:
- Chat over a local code snippet
- Answer questions about a pasted document
- Ground responses in retrieved database rows
- Summarise meeting notes / tickets

### `GET /health`

Returns availability of all three backends and current Copilot circuit state.

```json
{
  "status": "ok",
  "copilot_available": true,
  "cloud_available": true,
  "local_available": false,
  "circuit_state": "closed"
}
```

### `POST /admin/reset`

Manually reset both circuit breakers to `closed` (Copilot-preferred) state.

## âš™ï¸ Configuration (environment variables)

| Variable               | Default                   | Description                                |
|------------------------|---------------------------|--------------------------------------------|
| `GITHUB_COPILOT_TOKEN` | *(from VS Code config)*   | GitHub Copilot OAuth token (primary)       |
| `COPILOT_MODEL`        | `gpt-4o`                  | Copilot model name                         |
| `CLOUD_PROVIDER`       | `openai`                  | Secondary cloud provider                   |
| `CLOUD_MODEL`          | `gpt-4o-mini`             | Secondary cloud model                      |
| `OPENAI_API_KEY`       | *(required for cloud)*    | OpenAI API key (secondary)                 |
| `LOCAL_MODEL`          | `llama3`                  | Ollama model name (fallback)               |
| `OLLAMA_BASE_URL`      | `http://localhost:11434`  | Ollama service URL                         |
| `FAILURE_THRESHOLD`    | `3`                       | Failures before opening a tier's circuit   |
| `RECOVERY_TIMEOUT`     | `300`                     | Seconds before re-probing a failed tier    |

## ğŸ”„ Circuit Breaker State Machine (per tier)

```
CLOSED â”€â”€(N failures)â”€â”€â–º OPEN â”€â”€(timeout)â”€â”€â–º HALF-OPEN
  â–²                                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(success)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| State       | Behaviour                                             |
|-------------|-------------------------------------------------------|
| `closed`    | Tier healthy â€” requests routed here first             |
| `open`      | Tier down â€” skipped, next tier tried                  |
| `half_open` | Trial request sent to test recovery                   |

## ğŸ§© Language-Agnostic â€” Client Examples

The gateway is a plain HTTP REST API â€” call it from any language.
Ready-to-run examples live in `examples/`:

### Python (stdlib â€” no extra packages)

```python
import json, urllib.request

def query(prompt):
    payload = json.dumps({"prompt": prompt}).encode()
    req = urllib.request.Request(
        "http://localhost:8000/ai/query",
        data=payload, headers={"Content-Type": "application/json"}, method="POST"
    )
    with urllib.request.urlopen(req) as r:
        return json.loads(r.read())

def query_rag(prompt, documents):
    payload = json.dumps({"prompt": prompt, "documents": documents}).encode()
    req = urllib.request.Request(
        "http://localhost:8000/ai/query/rag",
        data=payload, headers={"Content-Type": "application/json"}, method="POST"
    )
    with urllib.request.urlopen(req) as r:
        return json.loads(r.read())
```

### Java (stdlib â€” Java 11+)

```java
HttpClient http = HttpClient.newHttpClient();
HttpRequest req = HttpRequest.newBuilder()
    .uri(URI.create("http://localhost:8000/ai/query"))
    .header("Content-Type", "application/json")
    .POST(HttpRequest.BodyPublishers.ofString("{\"prompt\":\"Hello\"}"))
    .build();
String body = http.send(req, HttpResponse.BodyHandlers.ofString()).body();
```

### C# (stdlib â€” .NET 6+)

```csharp
var client = new HttpClient();
var content = new StringContent(
    JsonSerializer.Serialize(new { prompt = "Hello" }),
    Encoding.UTF8, "application/json");
var resp = await client.PostAsync("http://localhost:8000/ai/query", content);
var json = await resp.Content.ReadAsStringAsync();
```

### Go (stdlib)

```go
body, _ := json.Marshal(map[string]string{"prompt": "Hello"})
resp, _ := http.Post("http://localhost:8000/ai/query",
    "application/json", bytes.NewReader(body))
result, _ := io.ReadAll(resp.Body)
```

> See `examples/` for complete working programs in all four languages.

## ğŸ’» VS Code + GitHub Copilot

Open this folder in VS Code:

```bash
code ai-gateway/
```

The `.vscode/settings.json` file pre-configures:
- Python test discovery (pytest)
- Copilot inline suggestions enabled
- Correct Python path for module imports

## ğŸ§ª Running Tests

```bash
cd ai-gateway
pytest tests/ -v
```

## ğŸ— Hardware Recommendations for Local (Fallback) Models

| RAM      | Suggested model |
|----------|-----------------|
| 8 GB     | `phi3` (3B)     |
| 16 GB    | `llama3` (7B)   |
| 32 GB    | `llama3:13b`    |
| GPU 8 GB+| Any â€” much faster |

## âš–ï¸ Tier Trade-offs

| Feature            | Copilot (Primary) | Cloud (Secondary) | Local (Fallback) |
|--------------------|-------------------|-------------------|------------------|
| Accuracy           | â­â­â­â­â­            | â­â­â­â­              | â­â­â­              |
| IDE integration    | âœ… Native           | âŒ                 | âŒ                |
| Internet required  | Yes               | Yes               | No               |
| Cost               | Copilot plan      | API-based         | Hardware-based   |
| Privacy            | GitHub            | External          | Fully private    |
