# AI Gateway â€” 3-Tier AI Failover (Copilot â†’ Cloud â†’ Local)

A production-ready AI gateway microservice that routes queries through a **3-tier failover chain**:

| Priority | Tier        | Backend               | When used                            |
|----------|-------------|-----------------------|--------------------------------------|
| 1        | **Primary** | GitHub Copilot        | Default â€” when Copilot is reachable  |
| 2        | **Secondary** | Cloud LLM (OpenAI) | When Copilot circuit is OPEN         |
| 3        | **Fallback** | Local Ollama          | When both cloud tiers are OPEN / offline |

Each cloud tier (Copilot & OpenAI) has an independent **circuit breaker** that opens after N consecutive failures and re-probes after a configurable timeout.

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     Your App        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ POST /ai/query
              AI Gateway Layer
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚             â”‚
  ğŸ¤– Copilot    ğŸŒ Cloud LLM   ğŸ’» Local LLM
  (Primary)    (Secondary)    (Fallback)
  GitHub       OpenAI, etc.   Ollama
```

## ğŸ“ Project Structure

```
ai-gateway/
â”œâ”€â”€ copilot_client.py   # GitHub Copilot client (primary)
â”œâ”€â”€ cloud_client.py     # Cloud LLM client (secondary â€” OpenAI)
â”œâ”€â”€ local_client.py     # Local LLM client (tertiary â€” Ollama)
â”œâ”€â”€ router.py           # 3-tier circuit-breaker router
â”œâ”€â”€ ai_gateway.py       # FastAPI REST service
â”œâ”€â”€ Dockerfile          # Container image
â”œâ”€â”€ docker-compose.yml  # Gateway + Ollama stack
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .vscode/
â”‚   â””â”€â”€ settings.json   # VS Code + Copilot settings
â””â”€â”€ tests/
    â””â”€â”€ test_gateway.py # Unit & integration tests
```

## ğŸš€ Quick Start

### Option 1 â€” Run locally

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set tokens (at minimum one cloud tier is required)
export GITHUB_COPILOT_TOKEN="<your-copilot-token>"  # primary
export OPENAI_API_KEY="sk-..."                        # secondary fallback

# 3. Start local Ollama (tertiary fallback â€” optional but recommended)
curl -fsSL https://ollama.com/install.sh | sh
ollama run llama3

# 4. Start the gateway
uvicorn ai_gateway:app --reload
```

### Option 2 â€” Docker Compose (recommended)

```bash
GITHUB_COPILOT_TOKEN=<token> OPENAI_API_KEY=sk-... docker compose up
```

The gateway is then available at `http://localhost:8000`.

### Getting a GitHub Copilot token

```bash
# Option A: GitHub CLI (recommended)
gh auth login --scopes copilot
gh auth token

# Option B: VS Code (token stored automatically after installing Copilot extension)
# Read from: ~/.config/github-copilot/hosts.json  (Linux/macOS)
```

## ğŸ”Œ API Reference

### `POST /ai/query`

```json
{
  "prompt": "What is the capital of France?"
}
```

Response:

```json
{
  "response": "The capital of France is Paris.",
  "backend": "copilot",
  "state": "closed"
}
```

| Field     | Description                                              |
|-----------|----------------------------------------------------------|
| `response`| The AI model's answer                                    |
| `backend` | `"copilot"`, `"cloud"`, or `"local"` â€” which tier replied |
| `state`   | Primary (Copilot) circuit state: `closed / open / half_open` |

### `GET /health`

Returns availability of all three backends and current Copilot circuit state.

```json
{
  "status": "ok",
  "copilot_available": true,
  "cloud_available": true,
  "local_available": false,
  "circuit_state": "closed"
}
```

### `POST /admin/reset`

Manually reset both circuit breakers to `closed` (Copilot-preferred) state.

## âš™ï¸ Configuration (environment variables)

| Variable               | Default                   | Description                                |
|------------------------|---------------------------|--------------------------------------------|
| `GITHUB_COPILOT_TOKEN` | *(from VS Code config)*   | GitHub Copilot OAuth token (primary)       |
| `COPILOT_MODEL`        | `gpt-4o`                  | Copilot model name                         |
| `CLOUD_PROVIDER`       | `openai`                  | Secondary cloud provider                   |
| `CLOUD_MODEL`          | `gpt-4o-mini`             | Secondary cloud model                      |
| `OPENAI_API_KEY`       | *(required for cloud)*    | OpenAI API key (secondary)                 |
| `LOCAL_MODEL`          | `llama3`                  | Ollama model name (fallback)               |
| `OLLAMA_BASE_URL`      | `http://localhost:11434`  | Ollama service URL                         |
| `FAILURE_THRESHOLD`    | `3`                       | Failures before opening a tier's circuit   |
| `RECOVERY_TIMEOUT`     | `300`                     | Seconds before re-probing a failed tier    |

## ğŸ”„ Circuit Breaker State Machine (per tier)

```
CLOSED â”€â”€(N failures)â”€â”€â–º OPEN â”€â”€(timeout)â”€â”€â–º HALF-OPEN
  â–²                                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(success)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| State       | Behaviour                                             |
|-------------|-------------------------------------------------------|
| `closed`    | Tier healthy â€” requests routed here first             |
| `open`      | Tier down â€” skipped, next tier tried                  |
| `half_open` | Trial request sent to test recovery                   |

## ğŸ’» VS Code + GitHub Copilot

Open this folder in VS Code:

```bash
code ai-gateway/
```

The `.vscode/settings.json` file pre-configures:
- Python test discovery (pytest)
- Copilot inline suggestions enabled
- Correct Python path for module imports

## ğŸ§ª Running Tests

```bash
cd ai-gateway
pytest tests/ -v
```

## ğŸ— Hardware Recommendations for Local (Fallback) Models

| RAM      | Suggested model |
|----------|-----------------|
| 8 GB     | `phi3` (3B)     |
| 16 GB    | `llama3` (7B)   |
| 32 GB    | `llama3:13b`    |
| GPU 8 GB+| Any â€” much faster |

## âš–ï¸ Tier Trade-offs

| Feature            | Copilot (Primary) | Cloud (Secondary) | Local (Fallback) |
|--------------------|-------------------|-------------------|------------------|
| Accuracy           | â­â­â­â­â­            | â­â­â­â­              | â­â­â­              |
| IDE integration    | âœ… Native           | âŒ                 | âŒ                |
| Internet required  | Yes               | Yes               | No               |
| Cost               | Copilot plan      | API-based         | Hardware-based   |
| Privacy            | GitHub            | External          | Fully private    |
